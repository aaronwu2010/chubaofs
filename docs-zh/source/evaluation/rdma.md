# RDMA和TCP的性能测试对比

## 配置

| 组件   | 配置      |
|------------|--------|
| CPU | Intel(R) Xeon(R) Gold 6330 CPU @ 2.00GHz  |
| 内存 | 512GB  |
| 网卡 | ConnectX-5  |

### fio测试命令例子
``` bash
fio --directory=/mnt/cubefs --direct=1 -iodepth 1 --thread --rw=read --ioengine=libaio --bs=4k --size=500M --numjobs=32 -group_reporting -name=testfile10
```

## 直写模式
在direct = 1的模式下，RDMA普遍具有优势。测试结果依赖当时的网络设备是否畅通，所以会有一定波动。我们认为在10%以内的误差是正常的。下面测试结果的带宽单位都是MB/s。其中绿色的部分都是提升在30%以上，蓝色的则是20-30%之间。
|threads |  direct	 |  case  |  rdma  |  tcp  |  compare  |
|--------|--------|--------|--------|--------|--------|
|1  |  1  |  4k write  |  16.5  |  15.3  |  7.84  |
|1  |  1  |  128k write  |  202  |  140  |  44.29  |
|1  |  1  |  1M write  |  153  |  136  |  12.50  |
|1  |  1  |  4k random write  |  4.67  |  4.53  |  3.09  |
|1  |  1  |  128k random write  |  116  |  123  |  -5.69  |
|1  |  1  |  1M random write  |  156  |  153  |  1.96  |
|32  |  1  |  4k write  |  216  |  173  |  24.86  |
|32  |  1  |  4k random write  |  97.2  |  102  |  -4.71  |
|32  |  1  |  64k write  |  1548  |  1592  |  -2.76  |
|32  |  1  |  64k random write  |  1054  |  982  |  7.33  |
|32  |  1  |  128k write  |  2469  |  2449  |  0.82  |
|32  |  1  |  128k random write  |  1818  |  1939  |  -6.24  |
|32  |  1  |  1M write  |  2611  |  2432  |  7.36  |
|32  |  1  |  1M random write  |  2449  |  2004  |  22.21  |

结论：在直写模式下，RDMA部分场景都有性能提升。部分场景和TCP基本持平。


## 直读模式
在小块单线程时，直读模式有比较大的性能提升。
线程数量达到32的时候，因为带宽接近了网卡的上限，所以两者的结果比较接近。受到当时网络状况的影响，测试结果会有上下波动，所以随机读的对比不明显。
表格中有2个对比数据是有所下降的，这个是受到RDMA创建新的链接代价比较大，另外就是当时网络性能波动的影响。总的来说，在读取模式下，RDMA的网络优势能够得到体现，所以会有一定的性能提升。
|threads | direct | case | rdma | tcp | compare |
|--------|--------|--------|--------|--------|--------|
|1 | 1 | 4k read | 35.8 | 29.5 | 21.36 |
|1 | 1 | 128k read | 519 | 395 | 31.39 |
|1 | 1 | 1M read | 581 | 396 | 46.72 |
|1 | 1 | 4k random read | 32.8 | 28.5 | 15.09 |
|1 | 1 | 128k random read | 504 | 406 | 24.14 |
|1 | 1 | 1M random read | 542 | 404 | 34.16 |
|32 | 1 | 4k read | 595 | 604 | -1.49 |
|32 | 1 | 4k random read | 526 | 591 | -11.00 |
|32 | 1 | 64k read | 4048 | 3108 | 30.24 | 
|32 | 1 | 64k random read | 3029 | 3099 | -2.26 |
|32 | 1 | 128k read | 3808 | 2815 | 35.28 |
|32 | 1 | 128k random read | 3421 | 3784 | -9.59 |
|32 | 1 | 1M read | 3394 | 2820 | 20.35 |
|32 | 1 | 1M random read | 3678 | 3697 | -0.51 |



## 缓存写模式
因为顺序写的队列最大是1024，RDMA的wq一般是32，最大并发是32，所以写的性能，RDMA比TCP降低30%以上。因为wq number是最大可以同时发送的数量，多路并发时都需要申请这个数量，所以这个值不能无限配置很大。
|threads | direct | case | rdma | tcp | compare |
|--------|--------|--------|--------|--------|--------|
|1 | 0 | 4k write | 74.1 | 88.9 | -16.65 |
|1 | 0 | 4k random write | 16.2 | 14.3 | 13.29 |
|1 | 0 | 128k write | 618 | 841 | -26.52 |
|1 | 0 | 1M write | 567 | 1000 | -43.30 |
|1 | 0 | 1M random write | 144 | 127 | 13.39 |
|32 | 0 | 4k write | 675 | 956 | -29.39 |
|32 | 0 | 4k random write | 271 | 251 | 7.97 |
|32 | 0 | 128k write | 1444 | 3068 | -52.93 |
|32 | 0 | 1M write | 1289 | 3072 | -58.04 |
|32 | 0 | 1M random write | 1133 | 1148 | -1.31 |


结论：
- 在缓存模式下，但是顺序写则是下降非常严重。
- 其它的随机读写则是基本持平。所以文件操作类型影响测试结果。
- 如果需要进一步优化，需要buffer动态扩展和收缩，需要一个客户端和服务的buffer管理交互过程，较为复杂，现阶段不考虑优化


## 缓存读模式
顺序读的rdma模式有比较大的性能提升，对比TCP如下。
|threads | direct | case | rdma | tcp | compare |
|--------|--------|--------|--------|--------|--------|
|1 | 0 | 4k read | 1039 | 889 | 14.44 |
|1 | 0 | 4k random read | 37.1 | 29.2 | 27.05 |
|1 | 0 | 128k read | 1387 | 915 | 51.58 |
|1 | 0 | 1M read | 1263 | 899 | 40.49 |
|1 | 0 | 1M random read | 435 | 332 | 31.02 |
|32 | 0 | 4k read | 2601 | 2592 | 0.35 |
|32 | 0 | 4k random read | 346 | 283 | 22.26 |
|32 | 0 | 128k read | 2971 | 2446 | 21.46 |
|32 | 0 | 1M read | 2827 | 2566 | 10.17 |
|32 | 0 | 1M random read | 2985 | 2362 | 26.38 |

分析：
缓存读不需要去写3份数据，不需要主从同步，也不需要更新metanode元数据。它仅仅需要从datanode上面读取一份数据。所以在这种场景下，单次网络链接的占比比较大。因为RDMA单纯带宽能够比TCP提升100%，所以上述表格中测试出来的结果有20%以上的提升是正常的。
结论：
顺序读read操作普遍有20%以上提升。
